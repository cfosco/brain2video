{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate videos by prompting with ground truth captions\n",
    "Oracle baseline for BOLDMoments videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import shutil\n",
    "import torch\n",
    "from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\n",
    "from diffusers.utils import export_to_video\n",
    "import os\n",
    "from utils import transform_vids_to_gifs, vid_to_gif, frames_to_vid\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load captions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annots['0001']: {'bmd_matrixfilename': 'vid_idx0001', 'MiT_url': 'https://data.csail.mit.edu/soundnet/actions3/wetting/0-0-1-6-7-2-8-0-17500167280.mp4', 'MiT_filename': 'wetting/0-0-1-6-7-2-8-0-17500167280.mp4', 'set': 'train', 'objects': ['red-breasted merganser', 'duck', 'American coot', 'goose', 'killer whale'], 'scenes': ['swimming hole', 'natural lake', 'watering hole', 'pond', 'ice floe'], 'actions': ['swimming', 'swimming', 'paddling', 'eating/feeding', 'swimming'], 'text_descriptions': ['A duck is swimming in a lake searching for food', 'A duck is floating atop the blue sparkly looking water.', 'A duck swims along in the water and pecks at the water.', 'A mallard is in the water alone swimming around and putting its beak in.', 'A duck swims in the daytime while pecking at the water.'], 'spoken_transcription': 'in a large mostly still body of water we see a duck swimming and pecking at the surface with his beak', 'memorability_score': 0.8147719988084737, 'memorability_decay': -0.00040570405564760616}\n"
     ]
    }
   ],
   "source": [
    "# Load captions\n",
    "annots = json.load(open('data/annotations.json', 'r')) # captions located in annots.values()[0]['text_descriptions']\n",
    "print(\"annots['0001']:\",annots['0001'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to generate videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_videos_from_annots_with_gradio_api(annots, start_from=0):\n",
    "    from gradio_client import Client\n",
    "\n",
    "    client = Client(\"https://fffiloni-zeroscope--x84m2.hf.space/\")\n",
    "    for i, a in annots.items():\n",
    "        if int(i) < start_from:\n",
    "            continue\n",
    "        for c in range(len(a['text_descriptions'])):\n",
    "            prompt = a['text_descriptions'][c]\n",
    "            print(prompt)\n",
    "            result = client.predict(\n",
    "                            prompt,\t# str in 'Prompt' Textbox component\n",
    "                            api_name=\"/zrscp\"\n",
    "            )\n",
    "\n",
    "            # Move video to correct folder\n",
    "            shutil.move(result, f'./oracle_gens/{i}_captionnumber{c}_{prompt}.mp4')\n",
    "\n",
    "            # Make gif\n",
    "            vid_to_gif(f'./oracle_gens/{i}_captionnumber{c}_{prompt}.mp4', f'./oracle_gens/{i}_captionnumber{c}_{prompt}.gif')\n",
    "            break\n",
    "\n",
    "def generate_videos_from_annots_with_local(annots, start_from=0, n_samples=5, save_frames=False):\n",
    "    pipe = DiffusionPipeline.from_pretrained(\"../zeroscope_v2_576w\", torch_dtype=torch.float16)\n",
    "    print(type(pipe))\n",
    "    pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "    # pipe.enable_model_cpu_offload()\n",
    "    pipe.to(\"cuda:0\")\n",
    "\n",
    "\n",
    "    for i, a in annots.items():\n",
    "        if int(i) < start_from:\n",
    "            continue\n",
    "        for n in range(n_samples):\n",
    "            for c in range(len(a['text_descriptions'])):\n",
    "                # if c==0: continue   # Skip first caption\n",
    "                prompt = a['text_descriptions'][c]\n",
    "                generator = torch.Generator(device=\"cuda\").manual_seed(n) # building len(prompts) generators with the exact same seed, so that only the prompt varies\n",
    "                print(\"Generating video for prompt:\", prompt)\n",
    "                video_frames = pipe(prompt, \n",
    "                                    num_inference_steps=20, \n",
    "                                    height=320, \n",
    "                                    width=320, \n",
    "                                    num_frames=24,\n",
    "                                    generator=generator).frames # video_frames is a list of frames, not a tensor\n",
    "\n",
    "                video_name = f'{i}_seed{n}_captionnumber{c}_{prompt.replace(\"/\",\"-\").replace(\" \", \"-\")}'\n",
    "\n",
    "                # Save frames\n",
    "                if save_frames:\n",
    "                    os.makedirs(f'./oracle_gens/frames/{video_name}', exist_ok=True)\n",
    "                    for k, frame in enumerate(video_frames):\n",
    "                        plt.imsave(f'./oracle_gens/frames/{video_name}/{(k+1):03d}.png', frame) # We save frames starting with index 1 to match original stimuli\n",
    "\n",
    "                # Make video and save\n",
    "                frames_to_vid(video_frames, f'./oracle_gens/mp4/{video_name}.mp4', fps=8)\n",
    "\n",
    "                # Make gif and save\n",
    "                vid_to_gif(f'./oracle_gens/mp4/{video_name}.mp4', f'./oracle_gens/gif/{video_name}.gif')\n",
    "\n",
    "\n",
    "generate_videos_from_annots_with_local(annots, start_from=1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename files\n",
    "\n",
    "# for f in sorted(os.listdir('./oracle_gens/mp4')):\n",
    "#     # rename {i}_captionnumber{c} to {i}_seed{s}_captionnumber{c}\n",
    "#     if f[5] == 'c':\n",
    "#         os.rename(f'./oracle_gens/mp4/{f}', f'./oracle_gens/mp4/{f[:4]}_seed0_{f[5:]}')\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convert_and_move_videos import convert_videos\n",
    "\n",
    "convert_videos('./oracle_gens/mp4', './oracle_gens/mp4_h264')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that all videos are there, including their converted and gif versions\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "def check_videos_exist(path_to_videos= './oracle_gens/', start_idx=1001, end_idx=1102):\n",
    "    \n",
    "    missing_vids = {}\n",
    "\n",
    "    folders_to_check = ['mp4', 'mp4_h264', 'gif', 'frames']\n",
    "\n",
    "    for folder in folders_to_check:\n",
    "        missing_vids[folder] = []\n",
    "        for v in tqdm(range(start_idx, end_idx+1), desc=f'Checking {folder} folder'):\n",
    "            for cap in range(5):\n",
    "                for seed in range(5):\n",
    "                    # Check for file that starts with {v}_seed{seed}_captionnumber{cap}\n",
    "                    pattern = f'{path_to_videos}/mp4/{v:04d}_seed{seed}_captionnumber{cap}_*'\n",
    "                    matching_files = glob.glob(pattern)\n",
    "                    if not matching_files:\n",
    "                        print(f'No files found matching pattern {pattern}')\n",
    "                        missing_vids[folder].append(pattern)\n",
    "\n",
    "\n",
    "    return missing_vids\n",
    "\n",
    "missing_vids = check_videos_exist(start_idx=1001, end_idx=1102) # all test videos are there!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy videos to NSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convert_and_move_videos import move_to_nsf\n",
    "    \n",
    "move_to_nsf('./oracle_gens/mp4_h264', '/data/vision/oliva/scratch/ejosephs/brainGen_eval/stimuli/oracle_gens_zeroscope')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
