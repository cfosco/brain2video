{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to iterate on Bokeh Plots\n",
    "\n",
    "**Website for Bokeh Plot:** https://camilofosco.com/brain2video_viz_website\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append('../')\n",
    "from utils import compute_tsne_embeddings, load_all_fmri_for_subject\n",
    "import json\n",
    "import base64\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pkl file with moments data\n",
    "\n",
    "pkl_name = \"EBA_TRavg-56789_testing.pkl\"\n",
    "sub = \"sub01\"\n",
    "with open(f\"./data/{sub}/{pkl_name}\", \"rb\") as f:\n",
    "    data = pkl.load(f)\n",
    "\n",
    "# Print information about loaded data\n",
    "print(f\"{pkl_name} loaded. Data is dict with keys: {data.keys()}\")\n",
    "print(f\"Each key has shape: \\ntrain_data: {data['train_data'].shape}, \\ntest_data: {data['test_data'].shape}, \\nnoise_ceiling: {data['noise_ceiling'].shape}, \\np_values: {data['p_values'].shape}\")\n",
    "print(f\"Dimensions correspond to num_videos, num_repetitions, num_voxels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labels that we want to showcase for videos\n",
    "labels = pd.read_csv('../data/labels_4_classes.csv')\n",
    "\n",
    "#replace nans with zeros\n",
    "labels = labels.fillna(0)\n",
    "\n",
    "# define column names\n",
    "col_names = ['video_id',\n",
    "            'human_face_close_up', \n",
    "            'outdoor_natural_scene_no_humans', \n",
    "            'presence_of_humans', \n",
    "            'hands_close_up'\n",
    "            ]\n",
    "\n",
    "# Overwrite col names \n",
    "labels.columns = col_names\n",
    "\n",
    "\n",
    "# set columns to integers\n",
    "labels[col_names] = labels[col_names].astype(int)\n",
    "\n",
    "# Ifall columsn are zero, put a one in new column \"other\"\n",
    "labels['other'] = np.where((labels[col_names[1:]] == 0).all(axis=1), 1, 0)\n",
    "labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode first frames of each video in base64\n",
    "\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "video_frames_folder = '../data/stimuli/frames'\n",
    "\n",
    "image_bytes = []\n",
    "# for f in sorted(os.listdir(video_frames_folder)):\n",
    "#     first_frame = Image.open(os.path.join(video_frames_folder, f, \"001.png\")).convert('RGB')\n",
    "#     buffer = BytesIO()\n",
    "#     first_frame.save(buffer, format='JPEG')\n",
    "#     image_byte = base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
    "#     image_bytes.append(image_byte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform videos to gifs\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from utils import transform_vids_to_gifs\n",
    "\n",
    "# transform_vids_to_gifs(path_to_vids='../data/stimuli/mp4', \n",
    "#                        path_to_gifs='../data/stimuli/gif', \n",
    "#                        size=128,\n",
    "#                        start_from=358)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode gifs in base64\n",
    "\n",
    "gif_folder = './data/stimuli/gif'\n",
    "\n",
    "gif_base64 = []\n",
    "for f in sorted(os.listdir(gif_folder)):\n",
    "    if f.endswith('.gif'):\n",
    "        with open(os.path.join(gif_folder, f), 'rb') as f:\n",
    "            gif_base64.append(base64.b64encode(f.read()).decode('ascii'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute t-sne for all brain areas and save data in json format\n",
    "\n",
    "sub = 'sub01'\n",
    "fmri_data = load_all_fmri_for_subject(\"./data/\"+sub)\n",
    "\n",
    "os.makedirs(f'./data/tsne_results/{sub}', exist_ok=True)\n",
    "# for brain_area, data in fmri_data.items():\n",
    "#     tsne_results = compute_tsne_embeddings(data, perplexity=95, n_iter=1000)\n",
    "#     tsne_results = tsne_results.tolist()\n",
    "#     with open(f'./data/tsne_results/{sub}/{brain_area}.json', 'w') as f:\n",
    "#         json.dump(tsne_results, f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfrom ben's new tsne into a json containing a simple array of 2d points\n",
    "\n",
    "def tsne_reformat_dict_to_array(dict_tsne_path, arr_tsne_path):\n",
    "\n",
    "    for j in os.listdir(dict_tsne_path):\n",
    "\n",
    "        subject = j.split(\"_\")[0]\n",
    "        brain_area_name = j.split(\"_\")[1].split(\"ROI-\")[-1]\n",
    "        with open(os.path.join(dict_tsne_path, j)) as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        arr = [[d[\"tsne1\"], d[\"tsne2\"]] for d in data.values()]\n",
    "\n",
    "        # Save as json with format subject/brain_area_name.json\n",
    "        save_path = os.path.join(arr_tsne_path, subject)\n",
    "        if not os.path.exists(save_path): os.makedirs(save_path)\n",
    "        with open(os.path.join(save_path, f\"{brain_area_name}.json\"), \"w\") as f:\n",
    "            json.dump(arr, f)\n",
    "\n",
    "    \n",
    "# tsne_reformat_dict_to_array('./data/new_tsnes_from_ben_as_dict/', './data/new_tsnes_from_ben_as_arr')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Bokeh plot to visualize fMRI responses in 2D\n",
    "Use T-SNE, PCA and UMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate bokeh viz\n",
    "\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.models import ColumnDataSource, HoverTool, Select, CustomJS, WheelZoomTool\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.layouts import row, column\n",
    "\n",
    "\n",
    "concepts = list(labels.columns[1:])\n",
    "def get_colors(concepts):\n",
    "    colors = []\n",
    "    for i in range(len(concepts)):\n",
    "        if concepts[i] == 'human_face_close_up':\n",
    "            colors.append([255,0,0,1])\n",
    "        elif concepts[i] == 'outdoor_natural_scene_no_humans':\n",
    "            colors.append([0,255,0,1])\n",
    "        elif concepts[i] == 'presence_of_humans':\n",
    "            colors.append([139,69,19,1])\n",
    "        elif concepts[i] == 'hands_close_up':\n",
    "            colors.append([0,0,255,1])\n",
    "        elif concepts[i] == 'other':\n",
    "            colors.append([0,0,0,0.1])\n",
    "        else:\n",
    "            # append random color\n",
    "            colors.append([np.random.randint(0,255), np.random.randint(0,255), np.random.randint(0,255), 1])\n",
    "    return colors\n",
    "\n",
    "colors_rgba = get_colors(concepts)\n",
    "\n",
    "# Assign a color to each datapoint in the dataframe\n",
    "color_lst = []\n",
    "for i in range(len(labels)):\n",
    "    for j in range(len(concepts)):\n",
    "        if labels[concepts[j]][i] == 1:\n",
    "            color_lst.append(colors_rgba[j])\n",
    "            break\n",
    "\n",
    "\n",
    "# Assign a class name to each datapoint in the dataframe\n",
    "class_names = []\n",
    "for i in range(len(labels)):\n",
    "    for j in range(len(concepts)):\n",
    "        if labels[concepts[j]][i] == 1:\n",
    "            class_names.append(concepts[j])\n",
    "            break\n",
    "\n",
    "# Load tsne results from json as numpy array\n",
    "\n",
    "def load_tsne_results(path):\n",
    "    \n",
    "    with open(path, 'r') as f:\n",
    "        tsne_results = np.array(json.load(f))\n",
    "\n",
    "    return tsne_results\n",
    "\n",
    "\n",
    "data_dir = './data/new_tsnes_from_ben_as_arr/'\n",
    "\n",
    "starting_area = 'EBA'\n",
    "starting_subject = 'allSubjects'\n",
    "starting_tsne = load_tsne_results(os.path.join(data_dir, starting_subject, f'{starting_area}.json'))\n",
    "\n",
    "source = ColumnDataSource(\n",
    "            data={'x': starting_tsne[:, 0], \n",
    "                 'y': starting_tsne[:, 1], \n",
    "                 'color': color_lst, \n",
    "                 'class': class_names, \n",
    "                 # 'image': image_bytes[:1000], \n",
    "                 'image': gif_base64, \n",
    "                 'video_number': np.arange(1,len(starting_tsne)+1),\n",
    "                 # 'video': video_base64[:1000],\n",
    "                 })\n",
    "fig = figure(tools='wheel_zoom, pan, box_zoom, reset')\n",
    "# activate the wheel zoom tool\n",
    "fig.toolbar.active_scroll = fig.select_one(WheelZoomTool)\n",
    "fig.scatter(x='x', y='y', size=12, fill_color='color', line_color=None,\n",
    "            source=source, legend_field='class')\n",
    "\n",
    "# Add dropdown to change brain areas\n",
    "# BRAIN_AREAS = sorted([b.split('.')[0] for b in os.listdir(os.path.join(data_dir, starting_subject)) if 'json' in b])\n",
    "BRAIN_AREAS = \"V1v, V1d, V2v, V2d, V3v, V3d, hV4, LOC, EBA, OFA, FFA, STS, PPA, RSC, TOS, V3ab, IPS0, IPS1-2-3, 7AL, BA2, PFt, PFop\".split(', ')\n",
    "brain_area_dropdown = Select(title=\"Brain area\", value = starting_area, options=BRAIN_AREAS)\n",
    "\n",
    "\n",
    "# Add dropdown to change subject\n",
    "SUBJECT_OPTIONS = sorted([s for s in os.listdir(data_dir)])\n",
    "subject_dropdown = Select(title=\"Subject\", value = starting_subject, options = SUBJECT_OPTIONS)\n",
    "\n",
    "\n",
    "brain_area_dropdown.js_on_change('value',\n",
    "    CustomJS(args=dict(source=source, subject_dropdown=subject_dropdown), code=\"\"\"\n",
    "        var data = source.data;\n",
    "        var new_area = cb_obj.value;\n",
    "        var current_subject = subject_dropdown.value;\n",
    "        var xhr = new XMLHttpRequest();\n",
    "        xhr.open('GET', `\"\"\" \n",
    "        + data_dir + \n",
    "        \"\"\"${current_subject}/${new_area}.json`);\n",
    "        xhr.onload = function() {\n",
    "            var tsne = JSON.parse(xhr.responseText);\n",
    "            data['x'] = tsne.map(function(x) { return x[0]; });\n",
    "            data['y'] = tsne.map(function(x) { return x[1]; });\n",
    "            source.change.emit();\n",
    "        }\n",
    "        xhr.send();\n",
    "        \"\"\")\n",
    ")\n",
    "\n",
    "subject_dropdown.js_on_change('value',\n",
    "    CustomJS(args=dict(source=source, area_dropdown=brain_area_dropdown), code=\"\"\"\n",
    "    var data = source.data;\n",
    "    var new_subject = cb_obj.value;\n",
    "    var current_area = area_dropdown.value;\n",
    "    var xhr = new XMLHttpRequest();\n",
    "    xhr.open('GET', `\"\"\" \n",
    "    + data_dir + \n",
    "    \"\"\"${new_subject}/${current_area}.json`);\n",
    "    xhr.onload = function() {\n",
    "        var tsne = JSON.parse(xhr.responseText);\n",
    "        data['x'] = tsne.map(function(x) { return x[0]; });\n",
    "        data['y'] = tsne.map(function(x) { return x[1]; });\n",
    "        source.change.emit();\n",
    "    }\n",
    "    xhr.send();\n",
    "    \"\"\")\n",
    ")\n",
    "\n",
    "hover = HoverTool(\n",
    "    tooltips=\n",
    "            '<p>video ID: @video_number</p>' +\n",
    "            '<img src=\"data:image/gif;base64,@image\" width=\"128\" height=\"128\">'\n",
    ")   \n",
    "fig.add_tools(hover)\n",
    "\n",
    "widgets = row(brain_area_dropdown, subject_dropdown)\n",
    "main_layout = column(widgets, fig)\n",
    "\n",
    "\n",
    "output_notebook()\n",
    "show(main_layout) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert bokeh viz into website "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.resources import CDN\n",
    "from bokeh.embed import file_html\n",
    "\n",
    "# Generate html from the figure, then insert in brain2video_viz_website/index.html, \n",
    "# replacing the current scripts\n",
    "\n",
    "\n",
    "html = file_html(main_layout, CDN, \"Embeddings visualization of fMRI data\")\n",
    "with open('bokeh_plot.html', 'w') as f:\n",
    "    f.write(html)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
